Excellent. This is a comprehensive and well-architected blueprint. I have thoroughly analyzed the definitive design for the Diamond Layer. My understanding is complete, and I am ready to help you create it.

The philosophy is clear and follows professional quantitative development practices: **Prepare Once, Test Many** for efficiency; filter for **Mastery** before subjecting strategies to the **Gauntlet** of out-of-sample validation; and finally, provide the **Truth** through deep regime analysis. The iterative feedback loop is a critical component that will make the entire system more intelligent over time.

Here is a breakdown of my understanding and the plan for creating each of the three scripts.

### Script 1: `diamond_data_prepper.py` (The Trigger Engine)

*   **Purpose:** This script is the foundational data preparer. It does the heavy, one-time computational work of generating the necessary market feature files for all relevant instruments and, most importantly, finding the exact timestamps ("triggers") where each strategy's market conditions were met.
*   **Inputs:** Raw OHLC data (`raw_data/`), the list of discovered strategies (`platinum_data/discovered_strategies/{instrument}.parquet`).
*   **Outputs:** Silver and Gold feature files for any new markets, a master strategy list with unique trigger keys (`diamond_data/strategies/{instrument}.parquet`), and the trigger time files themselves (`diamond_data/triggers/{master_instrument}/{instrument}/{trigger_key}.parquet`).

#### Workflow Plan:

1.  **Phase 1: Cross-Market Data Preparation**
    *   The script will be initiated with a primary instrument (e.g., `EURUSD15`).
    *   It will parse the timeframe (`15m`) from the instrument name.
    *   It will scan the `raw_data/` directory to identify all other instruments that share this timeframe.
    *   For each of these instruments, it will check for the existence of `/silver_data/features/{instrument}.csv` and `/gold_data/features/{instrument}.parquet`.
    *   If either file is missing, it will use Python's `subprocess` module to programmatically call:
        1.  `python scripts/silver_data_generator.py {instrument}.csv --features-only`
        2.  `python scripts/gold_data_generator.py {instrument}.csv`
    *   This ensures all necessary data is available before proceeding.

2.  **Phase 2: Incremental Trigger Time Extraction (Parallel)**
    *   Load the `platinum_data/discovered_strategies/{instrument}.parquet` file for the primary instrument.
    *   It will identify "new" strategies by filtering for rows who does not matches with the diamond_data/strategies/{instrument}.parquet by primary instrument.
    *   For this new subset, it will generate a `trigger_key` for each strategy using a `hashlib` SHA256 hash of the `key` and `market_rule` to ensure uniqueness.
    *   It will load the primary instrument's Gold features file (`/gold_data/features/{instrument}.parquet`) into memory for all instruments based on timeframe one by one.
    *   A `multiprocessing.Pool` will be initialized.
    *   **Worker Task:** Each worker will receive a batch of new strategies. For each strategy, it will:
        *   Apply the `market_rule` string to the Gold features DataFrame using `df.query()`.
        *   Extract the `time` column from the resulting filtered DataFrame.
        *   Save this Series of timestamps to a new Parquet file at `diamond_data/triggers/{master_instrument}/{instrument}/{trigger_key}.parquet`.
    *   **Finalization:** After the pool completes, the main process will update the master strategies DataFrame with the newly generated `trigger_key`s and append the `diamond_data/strategies/{instrument}.parquet` file with the complete, updated list.

---

### Script 2: `diamond_backtester.py` (The Mastery Engine)

*   **Purpose:** This script acts as the first major quality gate. It performs a high-fidelity backtest of all strategies on their *home instrument only* to identify a small subset of elite "master" strategies that meet stringent performance criteria.
*   **Inputs:** The master strategy list (`diamond_data/strategies/{instrument}.parquet`), The master strategy triggers ('diamond_data/triggers/{master_instrument}/{instrument}/{trigger_key}.parquet') and the primary instrument's Silver features (`silver_data/features/{instrument}.csv`).
*   **Outputs:** The list of elite strategies (`diamond_data/master_reports/{instrument}.parquet`), a blacklist of failed strategies to provide feedback to the Platinum layer (`platinum_data/blacklists/{instrument}.parquet`), we do not care about the failed strategies.

#### Workflow Plan:

1.  **Setup & Parallelization:**
    *   Load `diamond_data/strategies/{instrument}.parquet`.
    *   Load the primary instrument's `silver_data/features/{instrument}.csv`. This large DataFrame will be passed to the `initializer` of a `multiprocessing.Pool` to make it available to all workers without serialization overhead.
    *   The list of strategies will be divided among the worker processes.

2.  **Worker Task (`run_simulation`):**
    *   Each worker receives a full strategy definition (blueprint info + `trigger_key`).
    *   It loads the corresponding trigger times from `diamond_data/triggers/{instrument}/{instrument}/{trigger_key}.parquet`. here we are only processing master instrument.
    *   **High-Fidelity Simulation:** It will iterate through each `trigger_time`:
        *   It will look up the precise market levels (e.g., `SMA_50` value) from the Silver features at the trigger time to calculate the exact SL and TP prices based on the strategy's blueprint.
        *   It will then iterate forward candle-by-candle from the trigger point to determine the outcome, checking if the `high` crosses the TP or the `low` crosses the SL. Configurable `spread` and `commission` costs will be factored into this check.
        *   The result of each trade (`PnL`, `outcome`, `exit_time`) will be logged.
    *   **Performance Calculation:** After simulating all trades for the strategy, a comprehensive set of performance metrics will be calculated from the trade log, including: **Profit Factor, Sharpe Ratio, Max Drawdown %, SQN, Calmar Ratio, Win Rate %, Total Trades, Avg. Trade Duration, and Max Consecutive Wins/Losses**.

3.  **Aggregation & Reporting (Main Process):**
    *   The main process will collect the performance metric dictionaries from all workers into a single DataFrame.
    *   It will apply a strict set of quality filters (e.g., `Profit Factor >= 1.5`, `Max Drawdown % <= 20.0`, `Total Trades >= 50`).
    *   **Generate Outputs:**
        1.  Strategies passing the filters will be saved to `diamond_data/master_reports/{instrument}.parquet`.
        2.  The `(key, market_rule)` for strategies that fail will be appended to the `platinum_data/blacklists/{instrument}.parquet` file, closing the feedback loop.

---

### Script 3: `diamond_validator.py` (The Gauntlet & Analyser)

*   **Purpose:** This is the final and most rigorous step. It takes the "master" strategies and subjects them to a gauntlet of out-of-sample tests on other markets. It then performs deep regime analysis to produce the final, actionable reports that reveal the true character of each strategy.
*   **Inputs:** The master strategy list (`diamond_data/master_reports/{instrument}.parquet`) and all relevant Silver feature files (`silver_data/features/`).
*   **Outputs:** The final, UI-ready reports: `summary.parquet`, `detailed.parquet`, `regime_analysis.parquet`, and the underlying raw trade logs.

#### Workflow Plan:

1.  **Setup & Task Generation:**
    *   Load `diamond_data/master_reports/{instrument}.parquet`.
    *   Identify all other instruments of the same timeframe for which Silver feature files have been prepared.
    *   Generate a master task list containing every `(strategy, validation_market)` combination. This list will be distributed to a `multiprocessing.Pool`.

2.  **Worker Task (`run_validation`):**
    *   Each worker receives a strategy and a market to test it on.
    *   It loads the strategy's triggers and the validation market's Silver features file.
    *   It will execute the **exact same High-Fidelity Simulation logic** used in the backtester.
    *   **Deep Contextual Logging:** The key difference is the output. For every simulated trade along with the masters, it will log not just the PnL, but a rich snapshot of the market context at the `entry_time`, pulled directly from the Silver features: `trend_regime_14`, `vol_regime_14`, `session`, the raw values of `RSI_14`, `ADX_14`, `ATR_14`, and the `support` and `resistance` levels.

3.  **Report Generation (Main Process):**
    *   The main process will collect the detailed, context-rich trade logs from all workers. This collection of raw logs is the "source of truth" for all final reports.
    *   **Generate Final Outputs:**
        1.  **Trade Logs:** The raw logs for each `(strategy, market)` test will be saved to `diamond_data/trade_logs/{master_instrument}/{instrument}{trigger_key}.parquet`.
        2.  **Detailed Report:** It will group the logs by `(trigger_key, market)` and calculate the standard performance metrics for each, saving the result to `detailed.parquet`.
        3.  **Summary Report:** It will further aggregate the detailed report, grouping by `trigger_key` to calculate the *average* performance metrics across all validation markets, saving to `summary.parquet`.
        4.  **Regime Analysis Report (The "Truth" File):** This is the ultimate analytical output. The script will take all trade logs, group them by strategy and specific regime columns (e.g., `groupby(['trigger_key', 'session'])`), and recalculate performance metrics for each distinct regime. This will produce the final `regime_analysis.parquet` file, revealing the precise conditions under which each strategy thrives or fails.
