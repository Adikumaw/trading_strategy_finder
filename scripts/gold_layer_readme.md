# ü•á Gold Layer: The Machine Learning Preprocessor

This script represents the crucial final stage of data preparation in the pipeline. It acts as a specialized transformer, converting the human-readable, context-rich **Silver `features` dataset** into a purely numerical, normalized, and standardized format that is perfectly optimized for machine learning algorithms.

Its sole purpose is to "translate" market context into the mathematical language that ML models understand.

## What It Is

The Gold Layer is a highly focused, parallelized processing script. It takes the market feature files generated by the Silver Layer and applies a series of standard machine learning preprocessing techniques to every column.

The output is a **Gold Dataset**: a new version of the features file where every piece of information‚Äîfrom price levels to market sessions‚Äîhas been converted into a clean, numerical representation ready to be used as the `X` matrix in a model.

## Why It Is

Machine learning models, particularly tree-based models and neural networks, cannot work with raw, unprocessed data. They require inputs that are purely numerical and, ideally, scaled to a similar range. The Gold Layer is essential for several reasons:

1.  **Enabling Model Consumption:** It converts non-numeric data (like the string `London` for a session) into a numerical format (binary `0`s and `1`s) that models can process.
2.  **Creating Scale-Invariant Features:** By transforming absolute price levels (e.g., `SMA_50 = 1.25000`) into a relative distance from the current close price (e.g., `SMA_50_dist_norm = 0.001`), it allows the model to learn patterns that are independent of the instrument's price. This makes the discovered strategies far more robust and timeless.
3.  **Preventing Feature Dominance:** It applies `StandardScaler` to rescale features like `RSI` (which ranges from 0-100) and `MACD` (which has an unbounded range) to a common scale (mean of 0, standard deviation of 1). This prevents features with naturally large scales from unfairly dominating the learning process.
4.  **Noise Reduction:** It compresses the noisy, wide-ranging scores from candlestick patterns into a simple, discrete scale, capturing the core signal (bullish/bearish) while filtering out minor variations.

## How It Works

The script is a simple but powerful "assembly line" of data transformations, optimized for speed with parallel processing.

1.  **File Discovery:** It scans the `silver_data/features/` directory for any market feature files that have not yet been processed into Gold files.
2.  **Parallel Processing:** The user is prompted to enable multiprocessing. If enabled, the script creates a `Pool` of worker processes, assigning each input file to a different CPU core for simultaneous processing.
3.  **Core Transformation (`create_gold_features`):** Each worker process executes a series of transformations on its assigned DataFrame:
    - **Relational Transformation:** All columns representing an absolute price (e.g., `open`, `high`, `low`, `SMA_50`, `support`) are identified. Each one is converted into a new column representing its normalized distance from that candle's `close` price. The original price columns are then dropped.
    - **Categorical Encoding:** Text-based columns like `session`, `trend_regime`, and `vol_regime` are converted into multiple binary (0/1) columns using one-hot encoding.
    - **Pattern Compression:** The scores from all 61 `TA-Lib` candlestick patterns (ranging from -100 to 100) are binned into a simple 5-point scale (`-1.0, -0.5, 0.0, 0.5, 1.0`).
    - **Standardization:** All remaining numerical columns (e.g., `RSI`, `ADX`, `BB_width`) are scaled using `sklearn.preprocessing.StandardScaler`.
    - **Cleanup & Downcasting:** The script drops any remaining original columns, leaving only the newly created ML-ready features. Data types are downcast (e.g., `float64` to `float32`) to reduce the final file size.
4.  **Save Output:** The final, fully transformed DataFrame is saved to the `gold_data/features/` directory.

## üìÅ Folder Structure

```
project_root/
‚îú‚îÄ‚îÄ silver_data/
‚îÇ   ‚îî‚îÄ‚îÄ features/         # INPUT: Human-readable market features
‚îÇ       ‚îî‚îÄ‚îÄ XAUUSD15.csv
‚îÇ
‚îú‚îÄ‚îÄ gold_data/
‚îÇ   ‚îî‚îÄ‚îÄ features/         # OUTPUT: ML-ready, normalized features
‚îÇ       ‚îî‚îÄ‚îÄ XAUUSD15.csv
‚îÇ
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ gold_data_generator.py   # This script
```

## üìà Input & Output

### Input File Format (`silver_data/features/`)

The script consumes the `features` files generated by the Silver Layer. These are CSV files where each row is a candle and columns represent a mix of numerical and categorical market data.

### Output File Format (`gold_data/features/`)

The output is a CSV file of the same dimensions, but where every column (except for `time`) is a pure numerical feature, ready for machine learning.

| New Column Category      | Example Columns                         | Description                                                                   |
| :----------------------- | :-------------------------------------- | :---------------------------------------------------------------------------- |
| **`time`**               | `2023-10-18 14:30:00`                   | **Primary Key.** Untouched, used for joining with target data.                |
| **Relational Distances** | `SMA_50_dist_norm`, `support_dist_norm` | **(Scaled)** Normalized distances from the `close` price, with mean 0, std 1. |
| **Standard Indicators**  | `RSI_14`, `ADX`, `BB_width`             | **(Scaled)** All non-price indicators, standardized to mean 0, std 1.         |
| **Candlestick Patterns** | `CDLDOJI`, `CDLHAMMER`                  | **(Compressed)** Binned into a `[-1.0, -0.5, 0.0, 0.5, 1.0]` scale.           |
| **One-Hot Encoded**      | `session_London`, `trend_regime_trend`  | Binary (0 or 1) columns representing the market state.                        |

## üöÄ Possible Enhancements & Future Improvements

1.  **Consistent Scaling Across Datasets:** The current implementation fits a `StandardScaler` to each instrument's data independently. For some advanced modeling techniques (like training a single model on multiple instruments), it would be beneficial to fit the scaler on a "reference" dataset (e.g., EURUSD) and then use that _same fitted scaler_ to `transform` all other datasets. The Diamond Layer's `data_prepper` already does this, but it could be implemented here as well.
2.  **Alternative Scaling Methods:** While `StandardScaler` is a robust default, other scalers like `MinMaxScaler` (scales to a 0-1 range) or `RobustScaler` (less sensitive to outliers) could be beneficial for certain models. An option could be added to the script to choose the scaling method.
3.  **Feature Selection Integration:** This script is the perfect place to implement an automated feature selection step. After generating all features, techniques like Variance Thresholding (to remove constant features) or correlation analysis (to remove highly redundant features) could be applied to produce a more compact and efficient final feature set.
